{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xvu5Qvb3sYH"
      },
      "source": [
        "# **Puddle World**\n",
        "\n",
        "In this guide, we provide a step-by-step walkthrough on preparing the Puddle World environment for the Competition.\n",
        "\n",
        "Firstly, we cover the installation of the environment and related dependencies, both for local setup and utilization within a Colab notebook.\n",
        "\n",
        "Next, we detail the process of initializing the environment with desired configurations and demonstrate how to visualize its characteristics. The goal of the competition is to design an agent that does well in the five provided configurations. Therefore, we show you how to load all the various versions of the environment and inspect their characteristics.\n",
        "\n",
        "Given the objective of designing a competent agent, we present suggestions for deploying random, human, or untuned DQN agents within the environment. Observing all these different baseline behaviors serves as a valuable starting point for your work.\n",
        "\n",
        "Finally, it's time to design and train a single agent capable of superior performance across the five configurations of the environment. Testing the agent on these configurations, with the first being the default environment as described in the paper, is crucial. We provide instructions on saving results in a CSV file for submission, marking the beginning of your journey.\n",
        "\n",
        "Whether you're new to the field and eager to explore or an experienced practitioner aiming to refine your skills, this competition provides a platform to utilize your knowledge and creativity!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIhADOEiMMNG"
      },
      "source": [
        "# 1. Installing the Requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfH70vPMXZDI"
      },
      "source": [
        "In this section, you can find the necessary information for installing the environment and relavant libraries for your work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIJjN7wM36mV"
      },
      "source": [
        "# 2. Introduction to the Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgVQx5wsZ6No"
      },
      "source": [
        "The \"Puddle World\" environment provides an interface for reinforcement learning experiments. It features puddles and challenges agents to navigate to specified goal positions while avoiding the puddles with larger negative rewards.\n",
        "You can access key details like starting position, goal location, and action noise levels. By printing these attributes, you can gain insights into the environment's layout and develop strategies for training reinforcement learning algorithms effectively within the Puddle World domain.\n",
        "Here is the default Puddle World configuration inspired by the [original paper](/http://incompleteideas.net/papers/sutton-96.pdf):\n",
        "\n",
        "**Actions**\n",
        "\n",
        "There are four actions: up, down, right, and left.\n",
        "Each action moves approximately 0.05 in these directions. For the case where an action would take the agent out of the screen limits, the action does not move the agent instead.\n",
        "\n",
        "A random gaussian noise with standard deviation 0.01 is also added to the motion along both dimensions.\n",
        "\n",
        "**Reward**\n",
        "\n",
        "The reward is -1 for each time step, plus some additional penalty if the agent gets into the puddle.\n",
        "The penalty is -400 times the distance into the puddle (distance to the nearest edge).\n",
        "\n",
        "\n",
        "**Puddle Positions**\n",
        "\n",
        "The puddles's top-left position is [0.  , 0.85] and [0.35, 0.9] respectively, and the width and height for them is [0.55, 0.2 ] and [0.2, 0.6].\n",
        "\n",
        "**Start Position**\n",
        "\n",
        "The agent starts each episode at [0.2, 0.4].\n",
        "\n",
        "**Goal Position**\n",
        "\n",
        "The episode ends succesfully if the agent reaches [1.0, 1.0] which is the goal position.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-l8sBVaJgHZM"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import gym_puddle\n",
        "\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.dqn import MlpPolicy as DQNPolicy\n",
        "\n",
        "import time\n",
        "import json\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from IPython import display\n",
        "import pyvirtualdisplay\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCZgqX8EvWLo"
      },
      "source": [
        "Here are the printed details about the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nx5pViOgfWX",
        "outputId": "0b887fb4-0cf0-4e69-a11e-58fe5c3e6b1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start position: [0.2 0.4]\n",
            "goal position: [1. 1.]\n",
            "goal threshold: 0.1\n",
            "action noise: 0.01\n",
            "agent's thrust: 0.05\n",
            "puddle top left positions: [array([0.  , 0.85]), array([0.35, 0.9 ])]\n",
            "puddle widths and heights: [array([0.55, 0.2 ]), array([0.2, 0.6])]\n",
            "action space: [array([-0.05,  0.  ]), array([0.05, 0.  ]), array([ 0.  , -0.05]), array([0.  , 0.05])]\n",
            "observation space: Box(0.0, 1.0, (2,), float64)\n"
          ]
        }
      ],
      "source": [
        "  env = gym.make(\"PuddleWorld-v0\")\n",
        "\n",
        "  print(\"start position:\", env.get_wrapper_attr(\"start\"))\n",
        "  print(\"goal position:\", env.get_wrapper_attr(\"goal\"))\n",
        "  print(\"goal threshold:\", env.get_wrapper_attr(\"goal_threshold\"))\n",
        "  print(\"action noise:\", env.get_wrapper_attr(\"noise\"))\n",
        "  print(\"agent's thrust:\", env.get_wrapper_attr(\"thrust\"))\n",
        "  print(\"puddle top left positions:\", env.get_wrapper_attr(\"puddle_top_left\"))\n",
        "  print(\"puddle widths and heights:\", env.get_wrapper_attr(\"puddle_width\"))\n",
        "  print(\"action space:\", env.get_wrapper_attr(\"actions\"))\n",
        "  print(\"observation space:\", env.get_wrapper_attr(\"observation_space\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3zuQ4I4vdfm"
      },
      "source": [
        "Here is the visualization of the environment for the default configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_O2Y4pxziS-g"
      },
      "outputs": [],
      "source": [
        "#some functions to help the visualization and interaction wit the environment\n",
        "\n",
        "def visualize(frames, video_name = \"video.mp4\"):\n",
        "    # Saves the frames as an mp4 video using cv2\n",
        "    video_path = video_name\n",
        "    height, width, _ = frames[0].shape\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    video_writer = cv2.VideoWriter(video_path, fourcc, 30, (width, height))\n",
        "    for frame in frames:\n",
        "        video_writer.write(frame)\n",
        "    video_writer.release()\n",
        "\n",
        "def online_rendering(image):\n",
        "    #Visualize one frame of the image in a display\n",
        "    ax.axis('off')\n",
        "    img_with_frame = np.zeros((image.shape[0]+2, image.shape[1]+2, 3), dtype=np.uint8)\n",
        "    img_with_frame[1:-1, 1:-1, :] = image\n",
        "    ax.imshow(img_with_frame)\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)\n",
        "\n",
        "\n",
        "def prepare_display():\n",
        "  #Prepares display for onine rendering of the frames in the game\n",
        "  _display = pyvirtualdisplay.Display(visible=False,size=(1400, 900))\n",
        "  _ = _display.start()\n",
        "  fig, ax = plt.subplots(figsize=(5, 5))\n",
        "  ax.axis('off')\n",
        "\n",
        "\n",
        "def get_action():\n",
        "    action = None\n",
        "    while action not in [\"w\", \"a\", \"s\", \"d\", \"W\", \"A\", \"S\", \"D\"]:\n",
        "        action = input(\"Enter action (w/a/s/d): \")\n",
        "    if action == \"w\":\n",
        "        return 3\n",
        "    elif action == \"a\":\n",
        "        return 0\n",
        "    elif action == \"s\":\n",
        "        return 2\n",
        "    elif action == \"d\":\n",
        "        return 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "csQiaih2pfgr",
        "outputId": "1a3712db-f032-4467-fb6e-98c6eec5e982"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "obs, info = env.reset()\n",
        "image = env.render()\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "online_rendering(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: In the subsequent sections of this document, we have used the `prepare_display()` function, which starts a virtual display to show the online rendering of the frames of the game. This function is needed if you want to run the code on Colab, since Colab does not natively support virtual displays.\n",
        "\n",
        "However, if you are running your code on Windows, you don’t need this function. Instead, you can just rely on the `visualize()` function, which takes the list of frames as input and produces a video of the agent's performance in the game. Another way to visualize on Windows or Mac (or any system supporting virtual display windows) is to set the rendering mode to “human” at the beginning so that a window opens as the game starts and shows the online rendering.\n",
        "\n",
        "Here is an snippet of how you can do this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/siddhartha/Mac Desktop/AMII Hackathon/2024-upper-bound-ai-comp-1/myenv/lib/python3.10/site-packages/gymnasium/envs/registration.py:788: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='human' that is not in the possible render_modes ([]).\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"PuddleWorld-v0\", render_mode=\"human\") # you should set the render_mode to \"human\" to visualize the environment locally. If you are running this code snippet on colab, these lines won't work since colab doesn't support virtual display screens\n",
        "env.reset() #reset the environment to start a new episode\n",
        "env.render() #this will open a window to visualize the environment\n",
        "time.sleep(5) #wait for 5 seconds so that you can see the window\n",
        "env.close() #you should close the environment to close the virtual window at the end of your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpR--ha9O_mq"
      },
      "source": [
        "## 2.1 Accessing Different Environment Configurations\n",
        "\n",
        "Your task is to train an agent that can generalize well across different provided configurations of the environment. Each of these configurations feature different positions for puddles, which makes it challenging for the agent to find the most rewarding path to the goal.\n",
        "\n",
        "\n",
        "You can find these configurations in the `env_configs` folder of the repository. In order to access each version of the environment, you can provide the `.json` file indicating the environment details, and intitialize the puddle world as follows:\n",
        "(Note that if you are using colab, you should upload the configs to the files section of the colab)\n",
        "The puddle positions are different in these configurations, but other aspects of the environment remain the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "ZYDRdKW9O_QT",
        "outputId": "13785d2c-4f8e-4b48-c3c0-f622bca62dea"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "json_file = f\"gym_puddle/env_configs/pw4.json\"\n",
        "\n",
        "with open(json_file) as f:\n",
        "  env_setup = json.load(f)\n",
        "\n",
        "env = gym.make(\n",
        "  \"PuddleWorld-v0\",\n",
        "  start=env_setup[\"start\"],\n",
        "  goal=env_setup[\"goal\"],\n",
        "  goal_threshold=env_setup[\"goal_threshold\"],\n",
        "  noise=env_setup[\"noise\"],\n",
        "  thrust=env_setup[\"thrust\"],\n",
        "  puddle_top_left=env_setup[\"puddle_top_left\"],\n",
        "  puddle_width=env_setup[\"puddle_width\"],\n",
        ")\n",
        "\n",
        "obs, info = env.reset()\n",
        "image = env.render()\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "online_rendering(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2BaWj4cMaLW"
      },
      "source": [
        "# 3. Developing the Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtTuBkjbQ7MS"
      },
      "source": [
        "In preparation for the competition, we dive into the exploration of the RL framework through the introduction of three distinct agent types: random, human, and Deep Q-Network (DQN) agents.\n",
        "\n",
        "Each agent represents varying levels of sophistication and learning abilities within the Puddle World environment. This comprehensive investigation allows us to gain valuable insights into key aspects of the environment as well as exploring RL-related methods such as the utilization of deep reinforcement learning strategies, and related libraries.\n",
        "\n",
        "Through this exploration, you can familiarize yourself with the environment, grasp the details of the training loop, and become proficient in utilizing essential libraries such as Stable Baselines. This could be a starting point for building your own agent, either using existing libraries, and modifying them to your desires, or implementing new ideas from cutting-edge RL research!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg6H4SmN4HWz"
      },
      "source": [
        "## 3.1. Random Agent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdSBKNZyego-"
      },
      "source": [
        "In this section, a \"random agent\" is deployed to interact with the Puddle World environment. The environment is initialized and visualized, providing a starting point for the agent to explore the world. The random agent **selects actions randomly from the action space and applies them to the environment**. This process is repeated for a predetermined number of time steps (in this case, 10 steps). After each step, the agent's reward is accumulated, and the interaction is visualized through rendering the environment. Finally, the total reward earned in the total time steps is displayed, and the sequence of frames captured during the interaction is saved as a video for further analysis. This random agent serves as a baseline for you to see how the environment works and prepare for the performance of more sophisticated reinforcement learning algorithms in the Puddle World environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TV4sJQiXQi2i",
        "outputId": "e69d07a0-21e1-4c3f-a87a-700ef68c480b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " t: 0, observation: [0.14759479 0.40156839], reward: -1\n",
            " t: 1, observation: [0.15632535 0.46362078], reward: -1\n",
            " t: 2, observation: [0.09555847 0.46624159], reward: -1\n",
            " t: 3, observation: [0.10704538 0.49764418], reward: -1\n",
            " t: 4, observation: [0.11136758 0.44899226], reward: -1\n",
            " t: 5, observation: [0.15576357 0.46513777], reward: -1\n",
            " t: 6, observation: [0.23715241 0.46112585], reward: -1\n",
            " t: 7, observation: [0.15839042 0.45203164], reward: -1\n",
            " t: 8, observation: [0.21273543 0.45013314], reward: -1\n",
            " t: 9, observation: [0.16956376 0.43695851], reward: -1\n",
            "no episode finished in this run.\n"
          ]
        }
      ],
      "source": [
        "#prepare_display() #uncomment this line to see the online rendering of the environment frame by frame\n",
        "\n",
        "env = gym.make(\"PuddleWorld-v0\")\n",
        "\n",
        "obs, info = env.reset()\n",
        "total_reward = 0\n",
        "episode_rewards = []\n",
        "frames = []\n",
        "\n",
        "for time_step in range(10):\n",
        "    action = env.action_space.sample()  # take a random action\n",
        "    obs, reward, done, trunc, info = env.step(action)\n",
        "    total_reward += reward\n",
        "\n",
        "    image = env.render()\n",
        "    #online_rendering(image) #uncomment this line to see the online rendering of the environment frame by frame\n",
        "    frames.append(image)\n",
        "\n",
        "    print(f\" t: {time_step}, observation: {obs}, reward: {reward}\") #uncomment this line to see the environment-agent interaction details\n",
        "\n",
        "    if done:\n",
        "      print(f\"total reward in this episode: {total_reward}\")\n",
        "      episode_rewards.append(total_reward)\n",
        "      total_reward = 0\n",
        "      break\n",
        "\n",
        "env.close()\n",
        "\n",
        "if episode_rewards == []:\n",
        "  print(\"no episode finished in this run.\")\n",
        "else:\n",
        "  for i, reward in enumerate(episode_rewards):\n",
        "    print(f\"episode {i}: reward: {reward}\")\n",
        "\n",
        "visualize(frames, \"random.mp4\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp1z_dMMisxp"
      },
      "source": [
        "## 3.2. Human Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-uFDZVMkkEF"
      },
      "source": [
        "In this section, a \"Human Agent\" is introduced to interact with the Puddle World environment. The environment is initialized and visualized to provide a starting point for the human agent's interaction. Unlike the random agent, the human agent inputs actions directly from you. You are prompted to enter actions through the command line interface, allowing for real-time interaction. The key mapping is as follows:\n",
        "\n",
        "w: up\n",
        "a: left\n",
        "s: down\n",
        "d: right\n",
        "\n",
        "The agent's chosen actions are applied to the environment, and the resulting observations and rewards are displayed. This process is repeated for a predetermined number of time steps (in this case, 10 steps). After each step, the interaction is visualized through rendering the environment. The total reward earned in the total time steps is displayed, and the sequence of frames captured during the interaction is saved as a video for more analysis. This human agent setup enables you to actively engage with the environment and assess its dynamics and challenges from a human perspective."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUSYr_1BiutN",
        "outputId": "8775ec33-c8fd-4a93-88d0-5454639fff15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " t: 0, observation: [0.19414252 0.47274041], reward: -1\n",
            " t: 1, observation: [0.20488488 0.53600763], reward: -1\n",
            " t: 2, observation: [0.20399897 0.57971002], reward: -1\n",
            " t: 3, observation: [0.19892985 0.62077017], reward: -1\n",
            " t: 4, observation: [0.27105588 0.6227926 ], reward: -1\n",
            " t: 5, observation: [0.31531393 0.63265628], reward: -1\n",
            " t: 6, observation: [0.35289632 0.61945061], reward: -127.78024318597483\n",
            " t: 7, observation: [0.42482338 0.63323333], reward: -133.29333065273659\n"
          ]
        }
      ],
      "source": [
        "#prepare_display() #uncomment this line to see the online rendering of the environment frame by frame\n",
        "env = gym.make(\"PuddleWorld-v0\")\n",
        "\n",
        "obs, info = env.reset()\n",
        "total_reward = 0\n",
        "episode_rewards = []\n",
        "frames = []\n",
        "\n",
        "for time_step in range(10):\n",
        "    action = get_action()\n",
        "    obs, reward, done, trunc, info = env.step(action)\n",
        "    total_reward += reward\n",
        "\n",
        "    image = env.render()\n",
        "    # online_rendering(image) #uncomment this line to see the online rendering of the environment frame by frame\n",
        "    frames.append(image)\n",
        "\n",
        "    print(f\" t: {time_step}, observation: {obs}, reward: {reward}\")  #uncomment this line to see the environment-agent interaction details\n",
        "\n",
        "    if done:\n",
        "      print(f\"total reward in this episode: {total_reward}\")\n",
        "      episode_rewards.append(total_reward)\n",
        "      total_reward = 0\n",
        "      break\n",
        "\n",
        "env.close()\n",
        "\n",
        "if episode_rewards == []:\n",
        "  print(\"no episode finished in this run.\")\n",
        "else:\n",
        "  for i, reward in enumerate(episode_rewards):\n",
        "    print(f\"episode {i}: reward: {reward}\")\n",
        "\n",
        "visualize(frames, \"human.mp4\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Rew9gS45hi6"
      },
      "source": [
        "## 3.3. DQN Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAcvJ3mSlMT2"
      },
      "source": [
        "In this section, a  [ Deep Q-Network (DQN) agent](https://arxiv.org/abs/1312.5602) is utilized to interact with the Puddle World environment. The code demonstrates how to set up training with [Stable Baselines](https://stable-baselines3.readthedocs.io/en/master/), a popular library for reinforcement learning. The DQN model is initialized and trained using default hyperparameters, but you can explore and adjust these hyperparameters for optimal performance. After training, the trained model is loaded for evaluation. The environment is initialized, and the DQN agent's interaction with the environment is visualized. The agent selects actions based on the learned policy, and the environment responds accordingly. This process continues until the episode terminates. At the end of the episode, the total reward earned by the agent is displayed, and the sequence of frames captured during the interaction is saved as a video for further analysis. While the provided code serves as a starting point for training a DQN agent with stable baselines if you wish to use this library, you are encouraged to experiment with different hyperparameters to improve the agent's performance in navigating the complex dynamics of the Puddle World environment. We encourage you to explore more algorithms in the same library or build your agents from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBddK5ugiHTx",
        "outputId": "c6749a09-fae6-42b9-e79b-1052e282561b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 956       |\n",
            "|    ep_rew_mean      | -8.11e+03 |\n",
            "|    exploration_rate | 0.637     |\n",
            "| time/               |           |\n",
            "|    episodes         | 4         |\n",
            "|    fps              | 3082      |\n",
            "|    time_elapsed     | 1         |\n",
            "|    total_timesteps  | 3823      |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0001    |\n",
            "|    loss             | 17.7      |\n",
            "|    n_updates        | 930       |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 930       |\n",
            "|    ep_rew_mean      | -1.23e+04 |\n",
            "|    exploration_rate | 0.293     |\n",
            "| time/               |           |\n",
            "|    episodes         | 8         |\n",
            "|    fps              | 2876      |\n",
            "|    time_elapsed     | 2         |\n",
            "|    total_timesteps  | 7443      |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0001    |\n",
            "|    loss             | 12.8      |\n",
            "|    n_updates        | 1835      |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1.84e+03  |\n",
            "|    ep_rew_mean      | -9.66e+03 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 12        |\n",
            "|    fps              | 2606      |\n",
            "|    time_elapsed     | 8         |\n",
            "|    total_timesteps  | 22086     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0001    |\n",
            "|    loss             | 0.00313   |\n",
            "|    n_updates        | 5496      |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 2.65e+03  |\n",
            "|    ep_rew_mean      | -8.73e+03 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 16        |\n",
            "|    fps              | 2546      |\n",
            "|    time_elapsed     | 16        |\n",
            "|    total_timesteps  | 42344     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0001    |\n",
            "|    loss             | 6.53      |\n",
            "|    n_updates        | 10560     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 2.37e+03  |\n",
            "|    ep_rew_mean      | -7.35e+03 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 20        |\n",
            "|    fps              | 2532      |\n",
            "|    time_elapsed     | 18        |\n",
            "|    total_timesteps  | 47485     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0001    |\n",
            "|    loss             | 6.04      |\n",
            "|    n_updates        | 11846     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 2.06e+03  |\n",
            "|    ep_rew_mean      | -6.21e+03 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 24        |\n",
            "|    fps              | 2526      |\n",
            "|    time_elapsed     | 19        |\n",
            "|    total_timesteps  | 49513     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0001    |\n",
            "|    loss             | 0.00548   |\n",
            "|    n_updates        | 12353     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1.77e+03  |\n",
            "|    ep_rew_mean      | -5.38e+03 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 28        |\n",
            "|    fps              | 2526      |\n",
            "|    time_elapsed     | 19        |\n",
            "|    total_timesteps  | 49651     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0001    |\n",
            "|    loss             | 0.00125   |\n",
            "|    n_updates        | 12387     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1.57e+03  |\n",
            "|    ep_rew_mean      | -4.82e+03 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 32        |\n",
            "|    fps              | 2522      |\n",
            "|    time_elapsed     | 19        |\n",
            "|    total_timesteps  | 50260     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0001    |\n",
            "|    loss             | 0.00642   |\n",
            "|    n_updates        | 12539     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1.4e+03   |\n",
            "|    ep_rew_mean      | -4.37e+03 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 36        |\n",
            "|    fps              | 2521      |\n",
            "|    time_elapsed     | 19        |\n",
            "|    total_timesteps  | 50373     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0001    |\n",
            "|    loss             | 0.000826  |\n",
            "|    n_updates        | 12568     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1.26e+03  |\n",
            "|    ep_rew_mean      | -4.02e+03 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 40        |\n",
            "|    fps              | 2521      |\n",
            "|    time_elapsed     | 20        |\n",
            "|    total_timesteps  | 50505     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0001    |\n",
            "|    loss             | 4.04      |\n",
            "|    n_updates        | 12601     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 1.15e+03  |\n",
            "|    ep_rew_mean      | -3.73e+03 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 44        |\n",
            "|    fps              | 2520      |\n",
            "|    time_elapsed     | 20        |\n",
            "|    total_timesteps  | 50696     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0001    |\n",
            "|    loss             | 12.3      |\n",
            "|    n_updates        | 12648     |\n",
            "-----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.07e+03 |\n",
            "|    ep_rew_mean      | -3.5e+03 |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 48       |\n",
            "|    fps              | 2519     |\n",
            "|    time_elapsed     | 20       |\n",
            "|    total_timesteps  | 51130    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 6.52     |\n",
            "|    n_updates        | 12757    |\n",
            "----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 987       |\n",
            "|    ep_rew_mean      | -3.34e+03 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 52        |\n",
            "|    fps              | 2518      |\n",
            "|    time_elapsed     | 20        |\n",
            "|    total_timesteps  | 51303     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0001    |\n",
            "|    loss             | 0.00294   |\n",
            "|    n_updates        | 12800     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 918       |\n",
            "|    ep_rew_mean      | -3.15e+03 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 56        |\n",
            "|    fps              | 2517      |\n",
            "|    time_elapsed     | 20        |\n",
            "|    total_timesteps  | 51414     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0001    |\n",
            "|    loss             | 0.00175   |\n",
            "|    n_updates        | 12828     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 859       |\n",
            "|    ep_rew_mean      | -2.98e+03 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 60        |\n",
            "|    fps              | 2516      |\n",
            "|    time_elapsed     | 20        |\n",
            "|    total_timesteps  | 51539     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0001    |\n",
            "|    loss             | 0.0005    |\n",
            "|    n_updates        | 12859     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 808       |\n",
            "|    ep_rew_mean      | -2.84e+03 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 64        |\n",
            "|    fps              | 2515      |\n",
            "|    time_elapsed     | 20        |\n",
            "|    total_timesteps  | 51696     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0001    |\n",
            "|    loss             | 7.26      |\n",
            "|    n_updates        | 12898     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 763       |\n",
            "|    ep_rew_mean      | -2.72e+03 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 68        |\n",
            "|    fps              | 2514      |\n",
            "|    time_elapsed     | 20        |\n",
            "|    total_timesteps  | 51917     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0001    |\n",
            "|    loss             | 0.00118   |\n",
            "|    n_updates        | 12954     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 838       |\n",
            "|    ep_rew_mean      | -2.69e+03 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 72        |\n",
            "|    fps              | 2514      |\n",
            "|    time_elapsed     | 23        |\n",
            "|    total_timesteps  | 60306     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0001    |\n",
            "|    loss             | 0.00276   |\n",
            "|    n_updates        | 15051     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 796       |\n",
            "|    ep_rew_mean      | -2.61e+03 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 76        |\n",
            "|    fps              | 2514      |\n",
            "|    time_elapsed     | 24        |\n",
            "|    total_timesteps  | 60508     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0001    |\n",
            "|    loss             | 0.00506   |\n",
            "|    n_updates        | 15101     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 762       |\n",
            "|    ep_rew_mean      | -2.53e+03 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 80        |\n",
            "|    fps              | 2514      |\n",
            "|    time_elapsed     | 24        |\n",
            "|    total_timesteps  | 60952     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0001    |\n",
            "|    loss             | 4.51      |\n",
            "|    n_updates        | 15212     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 835       |\n",
            "|    ep_rew_mean      | -2.53e+03 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 84        |\n",
            "|    fps              | 2515      |\n",
            "|    time_elapsed     | 27        |\n",
            "|    total_timesteps  | 70126     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0001    |\n",
            "|    loss             | 0.00872   |\n",
            "|    n_updates        | 17506     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 800       |\n",
            "|    ep_rew_mean      | -2.45e+03 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 88        |\n",
            "|    fps              | 2514      |\n",
            "|    time_elapsed     | 27        |\n",
            "|    total_timesteps  | 70419     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0001    |\n",
            "|    loss             | 0.00139   |\n",
            "|    n_updates        | 17579     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 767       |\n",
            "|    ep_rew_mean      | -2.38e+03 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 92        |\n",
            "|    fps              | 2514      |\n",
            "|    time_elapsed     | 28        |\n",
            "|    total_timesteps  | 70559     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0001    |\n",
            "|    loss             | 0.00509   |\n",
            "|    n_updates        | 17614     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 737       |\n",
            "|    ep_rew_mean      | -2.31e+03 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 96        |\n",
            "|    fps              | 2514      |\n",
            "|    time_elapsed     | 28        |\n",
            "|    total_timesteps  | 70796     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0001    |\n",
            "|    loss             | 0.00551   |\n",
            "|    n_updates        | 17673     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 811       |\n",
            "|    ep_rew_mean      | -2.35e+03 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 100       |\n",
            "|    fps              | 2516      |\n",
            "|    time_elapsed     | 32        |\n",
            "|    total_timesteps  | 81106     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0001    |\n",
            "|    loss             | 0.000248  |\n",
            "|    n_updates        | 20251     |\n",
            "-----------------------------------\n",
            "-----------------------------------\n",
            "| rollout/            |           |\n",
            "|    ep_len_mean      | 871       |\n",
            "|    ep_rew_mean      | -2.12e+03 |\n",
            "|    exploration_rate | 0.05      |\n",
            "| time/               |           |\n",
            "|    episodes         | 104       |\n",
            "|    fps              | 2517      |\n",
            "|    time_elapsed     | 36        |\n",
            "|    total_timesteps  | 90961     |\n",
            "| train/              |           |\n",
            "|    learning_rate    | 0.0001    |\n",
            "|    loss             | 0.000508  |\n",
            "|    n_updates        | 22715     |\n",
            "-----------------------------------\n"
          ]
        }
      ],
      "source": [
        "#train the model, and save the trained model\n",
        "env = gym.make(\"PuddleWorld-v0\")\n",
        "dqn_model = DQN(DQNPolicy, env, verbose=1)\n",
        "dqn_model.learn(total_timesteps=int(1e5))\n",
        "dqn_model.save(\"dqn_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUI_JsgOtmYM",
        "outputId": "077b4a90-aaaa-4add-a2fe-734959d54a4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "no episode finished in this run.\n"
          ]
        }
      ],
      "source": [
        "dqn_model = DQN.load(\"dqn_model\")\n",
        "env = gym.make(\"PuddleWorld-v0\")\n",
        "\n",
        "obs, info = env.reset()\n",
        "\n",
        "# Create an empty list to store the frames\n",
        "frames = []\n",
        "episode_rewards = []\n",
        "\n",
        "for episode in range(1):\n",
        "  total_reward = 0\n",
        "  done = False\n",
        "  num_steps = 0\n",
        "\n",
        "  while not done and num_steps <=1000: # to avoid infinite loops for the untuned DQN we set a truncation limit, but you should make your agent sophisticated enough to avoid infinite-step episodes\n",
        "      num_steps +=1\n",
        "      action, _states = dqn_model.predict(obs)\n",
        "      obs, reward, done, trunc, info = env.step(action)\n",
        "      total_reward += reward\n",
        "      if done == True:\n",
        "        print(\"here\")\n",
        "\n",
        "      image = env.render()\n",
        "      frames.append(image)\n",
        "\n",
        "      if done:\n",
        "        print(f\"total reward in this episode: {total_reward}\")\n",
        "        episode_rewards.append(total_reward)\n",
        "        total_reward = 0\n",
        "        break\n",
        "\n",
        "env.close()\n",
        "\n",
        "if episode_rewards == []:\n",
        "  print(\"no episode finished in this run.\")\n",
        "else:\n",
        "  for i, reward in enumerate(episode_rewards):\n",
        "    print(f\"episode {i}: reward: {reward}\")\n",
        "\n",
        "visualize(frames, \"DQN.mp4\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvpUay3y5kYW"
      },
      "source": [
        "#4. Now It's Your Turn!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoUfMFHCl70O"
      },
      "source": [
        "Now, you're invited to dive into the world of reinforcement learning and take control of the Puddle World environment. We encourage you to become an active participant by modifying the provided code, tweaking parameters, and experimenting with different strategies. You can come up with your own agent ideas, or use pre-exisiting libraries, and adapt them to the problem.\n",
        "\n",
        "\n",
        "The goal is to build **a single agent that can generalize well across all the environment configurations.**\n",
        "Once you have built your agent, test your trained agent with 100 different seeds, each seed for one episode, in the five provided configurations, save the total reward in each episode for each configuration, and submit the results as a `.csv` file as indicated in the Kaggle platform. You can set different seeds for the environment with `env.reset(seed = n)`) with `n` being the number from 1 to 100.\n",
        "\n",
        " Whether you're a beginner eager to explore or an experienced practitioner seeking to refine your skills, this competition offers an opportunity to apply your knowledge and creativity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je02_uxV9N0c"
      },
      "source": [
        "## 4.1. Submission Format\n",
        "\n",
        "Here you can find a sample submission. Imagine that you have the episodic rewards saved in a list as shown in the below code. You can make a `submission.csv` file with the provided format as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAYlx07NY7sa",
        "outputId": "78bb77c1-65b2-48f6-872e-874ceecd99c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results saved successfully to submission.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming you have a list of 100 episodic rewards for each configuration\n",
        "# Example data structure: [(config1_rewards), (config2_rewards), ...]\n",
        "\n",
        "# Replace this example data with your actual rewards data\n",
        "episode_rewards_per_configuration = [\n",
        "     range(-120, -20),  # Configuration 1 rewards\n",
        "    range(-120, -20),   # Configuration 2 rewards\n",
        "    range(-120, -20),  # Configuration 3 rewards\n",
        "    range(-120, -20),  # Configuration 4 rewards\n",
        "   range(-120, -20),   # Configuration 5 rewards\n",
        "]\n",
        "\n",
        "# Define the column names\n",
        "columns = ['seed_ID', 'ep_reward_pw1', 'ep_reward_pw2', 'ep_reward_pw3', 'ep_reward_pw4', 'ep_reward_pw5']\n",
        "\n",
        "# Create a list of dictionaries to store data\n",
        "data = []\n",
        "\n",
        "# Populate the list with episode IDs and rewards\n",
        "for episode_id in range(1, 101):  # Assuming 100 episodes\n",
        "    row_data = {'seed_ID': episode_id}\n",
        "    for i, rewards in enumerate(episode_rewards_per_configuration):\n",
        "        row_data[columns[i + 1]] = rewards[episode_id - 1]\n",
        "    data.append(row_data)\n",
        "\n",
        "# Create DataFrame from the list of dictionaries\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Define the file name for saving the results\n",
        "csv_file_name = \"submission.csv\"\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv(csv_file_name, index=False)\n",
        "\n",
        "print(\"Results saved successfully to\", csv_file_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1mwZltVat-9",
        "outputId": "fc069386-ed4e-4daf-ab23-c458b0e93efa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    seed_ID  ep_reward_pw1  ep_reward_pw2  ep_reward_pw3  ep_reward_pw4  \\\n",
            "0         1           -120           -120           -120           -120   \n",
            "1         2           -119           -119           -119           -119   \n",
            "2         3           -118           -118           -118           -118   \n",
            "3         4           -117           -117           -117           -117   \n",
            "4         5           -116           -116           -116           -116   \n",
            "..      ...            ...            ...            ...            ...   \n",
            "95       96            -25            -25            -25            -25   \n",
            "96       97            -24            -24            -24            -24   \n",
            "97       98            -23            -23            -23            -23   \n",
            "98       99            -22            -22            -22            -22   \n",
            "99      100            -21            -21            -21            -21   \n",
            "\n",
            "    ep_reward_pw5  \n",
            "0            -120  \n",
            "1            -119  \n",
            "2            -118  \n",
            "3            -117  \n",
            "4            -116  \n",
            "..            ...  \n",
            "95            -25  \n",
            "96            -24  \n",
            "97            -23  \n",
            "98            -22  \n",
            "99            -21  \n",
            "\n",
            "[100 rows x 6 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the file name for loading the results\n",
        "csv_file_name = \"submission.csv\"\n",
        "\n",
        "# Load the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv(csv_file_name)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHLrXbuf7FH4"
      },
      "source": [
        "# 6. Fun Expedition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwxRkDNivkUi"
      },
      "source": [
        "You can also modify the properties of the environment such as the initial state, puddle locations, and more. You can play around with the environment and challenge your agent to tackle the hardest versions of this environment!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "EkxQLUa_ib18",
        "outputId": "e85d34c3-d611-4e4e-e12b-1100cc9e3ae5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "env = gym.make(\n",
        "        \"PuddleWorld-v0\",\n",
        "        start=[0.5,0.5],\n",
        "        goal=[1.,1.],\n",
        "        goal_threshold=0.1,\n",
        "        noise=0.01,\n",
        "        thrust=0.05,\n",
        "        puddle_top_left=[[0.1  , 0.8],[0.5, 0.1 ]],\n",
        "        puddle_width=[[0.1, 0.1 ],[0.3, 0.1]]\n",
        "    )\n",
        "\n",
        "obs, info = env.reset()\n",
        "image = env.render()\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "online_rendering(image)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
